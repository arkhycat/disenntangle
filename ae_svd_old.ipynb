{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# torch.manual_seed(111)\n",
    "# torch.cuda.manual_seed(111)\n",
    "# np.random.seed(111)\n",
    "# random.seed(111)\n",
    "# torch.backends.cudnn.enabled=False\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncc = 2\n",
    "n_epochs = 10\n",
    "bn_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1797: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, training loss 0.2883, val loss 0.2796, block_reg 0.94, dropout rate 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/diseNN/models.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.in_mask = torch.tensor(mask, requires_grad=False).to(self.weight.get_device())\n",
      "/diseNN/models.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.out_mask = torch.tensor(mask, requires_grad=False).to(self.weight.get_device())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned to 199 neurons\n",
      "Batch 101, training loss 0.0588, val loss 0.0616, block_reg 0.93, dropout rate 0.50\n",
      "Batch 201, training loss 0.0439, val loss 0.0419, block_reg 0.92, dropout rate 0.50\n",
      "Batch 301, training loss 0.0387, val loss 0.0378, block_reg 0.92, dropout rate 0.50\n",
      "Pruned to 198 neurons\n",
      "Batch 401, training loss 0.0308, val loss 0.0337, block_reg 0.92, dropout rate 0.50\n",
      "Batch 501, training loss 0.0314, val loss 0.0274, block_reg 0.92, dropout rate 0.50\n",
      "Batch 601, training loss 0.0271, val loss 0.0273, block_reg 0.91, dropout rate 0.50\n",
      "Batch 701, training loss 0.0254, val loss 0.0253, block_reg 0.91, dropout rate 0.50\n",
      "Pruned to 197 neurons\n",
      "Batch 801, training loss 0.0233, val loss 0.0228, block_reg 0.91, dropout rate 0.50\n",
      "Batch 901, training loss 0.0214, val loss 0.0233, block_reg 0.90, dropout rate 0.50\n",
      "Batch 1001, training loss 0.0207, val loss 0.0194, block_reg 0.90, dropout rate 0.50\n",
      "Batch 1101, training loss 0.0228, val loss 0.0197, block_reg 0.89, dropout rate 0.50\n",
      "Pruned to 196 neurons\n",
      "Batch 1201, training loss 0.0189, val loss 0.0168, block_reg 0.89, dropout rate 0.50\n",
      "Batch 1301, training loss 0.0189, val loss 0.0191, block_reg 0.88, dropout rate 0.50\n",
      "Batch 1401, training loss 0.0209, val loss 0.0185, block_reg 0.88, dropout rate 0.50\n",
      "Batch 1501, training loss 0.0202, val loss 0.0161, block_reg 0.88, dropout rate 0.50\n",
      "Pruned to 195 neurons\n",
      "Batch 1601, training loss 0.0182, val loss 0.0186, block_reg 0.87, dropout rate 0.50\n",
      "Batch 1701, training loss 0.0181, val loss 0.0160, block_reg 0.87, dropout rate 0.50\n",
      "Batch 1801, training loss 0.0161, val loss 0.0163, block_reg 0.87, dropout rate 0.50\n",
      "Train epoch 1, loss 0.030140009688337643\n",
      "Batch 1, training loss 0.0162, val loss 0.0162, block_reg 0.86, dropout rate 0.50\n",
      "Pruned to 194 neurons\n",
      "Batch 101, training loss 0.0188, val loss 0.0147, block_reg 0.86, dropout rate 0.50\n",
      "Batch 201, training loss 0.0169, val loss 0.0151, block_reg 0.86, dropout rate 0.50\n",
      "Batch 301, training loss 0.0157, val loss 0.0152, block_reg 0.85, dropout rate 0.50\n",
      "Pruned to 193 neurons\n",
      "Batch 401, training loss 0.0168, val loss 0.0145, block_reg 0.85, dropout rate 0.50\n",
      "Batch 501, training loss 0.0169, val loss 0.0136, block_reg 0.85, dropout rate 0.50\n",
      "Batch 601, training loss 0.0147, val loss 0.0134, block_reg 0.85, dropout rate 0.50\n",
      "Batch 701, training loss 0.0186, val loss 0.0140, block_reg 0.84, dropout rate 0.50\n",
      "Pruned to 192 neurons\n",
      "Batch 801, training loss 0.0164, val loss 0.0152, block_reg 0.84, dropout rate 0.50\n",
      "Batch 901, training loss 0.0147, val loss 0.0144, block_reg 0.84, dropout rate 0.50\n",
      "Batch 1001, training loss 0.0162, val loss 0.0131, block_reg 0.84, dropout rate 0.50\n",
      "Batch 1101, training loss 0.0147, val loss 0.0137, block_reg 0.83, dropout rate 0.50\n",
      "Pruned to 191 neurons\n",
      "Batch 1201, training loss 0.0142, val loss 0.0124, block_reg 0.83, dropout rate 0.50\n",
      "Batch 1301, training loss 0.0150, val loss 0.0129, block_reg 0.83, dropout rate 0.50\n",
      "Batch 1401, training loss 0.0147, val loss 0.0132, block_reg 0.83, dropout rate 0.50\n",
      "Batch 1501, training loss 0.0135, val loss 0.0134, block_reg 0.83, dropout rate 0.50\n",
      "Pruned to 190 neurons\n",
      "Batch 1601, training loss 0.0151, val loss 0.0144, block_reg 0.82, dropout rate 0.50\n",
      "Batch 1701, training loss 0.0168, val loss 0.0132, block_reg 0.82, dropout rate 0.50\n",
      "Batch 1801, training loss 0.0161, val loss 0.0134, block_reg 0.82, dropout rate 0.50\n",
      "Train epoch 2, loss 0.015586121476689974\n",
      "Batch 1, training loss 0.0150, val loss 0.0133, block_reg 0.82, dropout rate 0.50\n",
      "Pruned to 189 neurons\n",
      "Batch 101, training loss 0.0141, val loss 0.0126, block_reg 0.81, dropout rate 0.50\n",
      "Batch 201, training loss 0.0254, val loss 0.0225, block_reg 0.82, dropout rate 0.50\n",
      "Batch 301, training loss 0.0153, val loss 0.0139, block_reg 0.82, dropout rate 0.50\n",
      "Pruned to 188 neurons\n",
      "Batch 401, training loss 0.0162, val loss 0.0130, block_reg 0.81, dropout rate 0.50\n",
      "Batch 501, training loss 0.0151, val loss 0.0135, block_reg 0.81, dropout rate 0.50\n",
      "Batch 601, training loss 0.0154, val loss 0.0131, block_reg 0.81, dropout rate 0.50\n",
      "Batch 701, training loss 0.0145, val loss 0.0130, block_reg 0.81, dropout rate 0.50\n",
      "Pruned to 187 neurons\n",
      "Batch 801, training loss 0.0131, val loss 0.0114, block_reg 0.81, dropout rate 0.50\n",
      "Batch 901, training loss 0.0159, val loss 0.0131, block_reg 0.80, dropout rate 0.50\n",
      "Batch 1001, training loss 0.0147, val loss 0.0116, block_reg 0.80, dropout rate 0.50\n",
      "Batch 1101, training loss 0.0132, val loss 0.0124, block_reg 0.80, dropout rate 0.50\n",
      "Pruned to 186 neurons\n",
      "Batch 1201, training loss 0.0153, val loss 0.0116, block_reg 0.80, dropout rate 0.50\n",
      "Batch 1301, training loss 0.0127, val loss 0.0116, block_reg 0.80, dropout rate 0.50\n",
      "Batch 1401, training loss 0.0126, val loss 0.0107, block_reg 0.79, dropout rate 0.50\n",
      "Batch 1501, training loss 0.0138, val loss 0.0125, block_reg 0.79, dropout rate 0.50\n",
      "Pruned to 185 neurons\n",
      "Batch 1601, training loss 0.0142, val loss 0.0117, block_reg 0.79, dropout rate 0.50\n",
      "Batch 1701, training loss 0.0129, val loss 0.0133, block_reg 0.79, dropout rate 0.50\n",
      "Batch 1801, training loss 0.0148, val loss 0.0109, block_reg 0.79, dropout rate 0.50\n",
      "Train epoch 3, loss 0.014707840996483961\n",
      "Batch 1, training loss 0.0125, val loss 0.0105, block_reg 0.79, dropout rate 0.50\n",
      "Pruned to 184 neurons\n",
      "Batch 101, training loss 0.0126, val loss 0.0112, block_reg 0.78, dropout rate 0.50\n",
      "Batch 201, training loss 0.0130, val loss 0.0114, block_reg 0.78, dropout rate 0.50\n",
      "Batch 301, training loss 0.0128, val loss 0.0118, block_reg 0.78, dropout rate 0.50\n",
      "Pruned to 183 neurons\n",
      "Batch 401, training loss 0.0125, val loss 0.0108, block_reg 0.78, dropout rate 0.50\n",
      "Batch 501, training loss 0.0136, val loss 0.0116, block_reg 0.78, dropout rate 0.50\n",
      "Batch 601, training loss 0.0133, val loss 0.0117, block_reg 0.77, dropout rate 0.50\n",
      "Batch 701, training loss 0.0130, val loss 0.0106, block_reg 0.77, dropout rate 0.50\n",
      "Pruned to 182 neurons\n",
      "Batch 801, training loss 0.0143, val loss 0.0114, block_reg 0.77, dropout rate 0.50\n",
      "Batch 901, training loss 0.0103, val loss 0.0106, block_reg 0.77, dropout rate 0.50\n",
      "Batch 1001, training loss 0.0120, val loss 0.0106, block_reg 0.77, dropout rate 0.50\n",
      "Batch 1101, training loss 0.0120, val loss 0.0116, block_reg 0.77, dropout rate 0.50\n",
      "Pruned to 181 neurons\n",
      "Batch 1201, training loss 0.0130, val loss 0.0113, block_reg 0.76, dropout rate 0.50\n",
      "Batch 1301, training loss 0.0111, val loss 0.0111, block_reg 0.76, dropout rate 0.50\n",
      "Batch 1401, training loss 0.0136, val loss 0.0110, block_reg 0.76, dropout rate 0.50\n",
      "Batch 1501, training loss 0.0114, val loss 0.0102, block_reg 0.76, dropout rate 0.50\n",
      "Pruned to 180 neurons\n",
      "Batch 1601, training loss 0.0147, val loss 0.0096, block_reg 0.76, dropout rate 0.50\n",
      "Batch 1701, training loss 0.0122, val loss 0.0096, block_reg 0.76, dropout rate 0.50\n",
      "Batch 1801, training loss 0.0295, val loss 0.0417, block_reg 0.76, dropout rate 0.50\n",
      "Train epoch 4, loss 0.013464076664547126\n",
      "Batch 1, training loss 0.0204, val loss 0.0183, block_reg 0.79, dropout rate 0.50\n",
      "Pruned to 179 neurons\n",
      "Batch 101, training loss 0.0158, val loss 0.0153, block_reg 0.79, dropout rate 0.50\n",
      "Batch 201, training loss 0.0131, val loss 0.0117, block_reg 0.79, dropout rate 0.50\n",
      "Batch 301, training loss 0.0109, val loss 0.0105, block_reg 0.79, dropout rate 0.50\n",
      "Pruned to 178 neurons\n",
      "Batch 401, training loss 0.0111, val loss 0.0108, block_reg 0.78, dropout rate 0.50\n",
      "Batch 501, training loss 0.0139, val loss 0.0092, block_reg 0.78, dropout rate 0.50\n",
      "Batch 601, training loss 0.0116, val loss 0.0090, block_reg 0.78, dropout rate 0.50\n",
      "Batch 701, training loss 0.0124, val loss 0.0101, block_reg 0.78, dropout rate 0.50\n",
      "Pruned to 177 neurons\n",
      "Batch 801, training loss 0.0106, val loss 0.0095, block_reg 0.78, dropout rate 0.50\n",
      "Batch 901, training loss 0.0098, val loss 0.0098, block_reg 0.78, dropout rate 0.50\n",
      "Batch 1001, training loss 0.0098, val loss 0.0096, block_reg 0.78, dropout rate 0.50\n",
      "Batch 1101, training loss 0.0114, val loss 0.0085, block_reg 0.77, dropout rate 0.50\n",
      "Pruned to 176 neurons\n",
      "Batch 1201, training loss 0.0106, val loss 0.0085, block_reg 0.77, dropout rate 0.50\n",
      "Batch 1301, training loss 0.0181, val loss 0.0170, block_reg 0.78, dropout rate 0.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1401, training loss 0.0139, val loss 0.0131, block_reg 0.77, dropout rate 0.50\n",
      "Batch 1501, training loss 0.0137, val loss 0.0108, block_reg 0.77, dropout rate 0.50\n",
      "Pruned to 175 neurons\n",
      "Batch 1601, training loss 0.0129, val loss 0.0099, block_reg 0.77, dropout rate 0.50\n",
      "Batch 1701, training loss 0.0133, val loss 0.0104, block_reg 0.77, dropout rate 0.50\n",
      "Batch 1801, training loss 0.0133, val loss 0.0092, block_reg 0.77, dropout rate 0.50\n",
      "Train epoch 5, loss 0.013056274606784185\n",
      "Batch 1, training loss 0.0112, val loss 0.0086, block_reg 0.76, dropout rate 0.50\n",
      "Pruned to 174 neurons\n",
      "Batch 101, training loss 0.0149, val loss 0.0119, block_reg 0.76, dropout rate 0.50\n",
      "Batch 201, training loss 0.0156, val loss 0.0097, block_reg 0.76, dropout rate 0.50\n",
      "Batch 301, training loss 0.0106, val loss 0.0078, block_reg 0.76, dropout rate 0.50\n",
      "Pruned to 173 neurons\n",
      "Batch 401, training loss 0.0130, val loss 0.0084, block_reg 0.76, dropout rate 0.50\n",
      "Batch 501, training loss 0.0089, val loss 0.0088, block_reg 0.76, dropout rate 0.50\n",
      "Batch 601, training loss 0.0094, val loss 0.0088, block_reg 0.76, dropout rate 0.50\n",
      "Batch 701, training loss 0.0097, val loss 0.0085, block_reg 0.76, dropout rate 0.50\n",
      "Pruned to 172 neurons\n",
      "Batch 801, training loss 0.0097, val loss 0.0083, block_reg 0.75, dropout rate 0.50\n",
      "Batch 901, training loss 0.0093, val loss 0.0091, block_reg 0.75, dropout rate 0.50\n",
      "Batch 1001, training loss 0.0099, val loss 0.0090, block_reg 0.75, dropout rate 0.50\n",
      "Batch 1101, training loss 0.0112, val loss 0.0083, block_reg 0.75, dropout rate 0.50\n",
      "Pruned to 171 neurons\n",
      "Batch 1201, training loss 0.0109, val loss 0.0086, block_reg 0.75, dropout rate 0.50\n",
      "Batch 1301, training loss 0.0099, val loss 0.0094, block_reg 0.75, dropout rate 0.50\n",
      "Batch 1401, training loss 0.0092, val loss 0.0070, block_reg 0.75, dropout rate 0.50\n",
      "Batch 1501, training loss 0.0107, val loss 0.0090, block_reg 0.75, dropout rate 0.50\n",
      "Pruned to 170 neurons\n",
      "Batch 1601, training loss 0.0114, val loss 0.0099, block_reg 0.75, dropout rate 0.50\n",
      "Batch 1701, training loss 0.0092, val loss 0.0081, block_reg 0.75, dropout rate 0.50\n",
      "Batch 1801, training loss 0.0096, val loss 0.0090, block_reg 0.74, dropout rate 0.50\n",
      "Train epoch 6, loss 0.01107046544700861\n",
      "Batch 1, training loss 0.0120, val loss 0.0084, block_reg 0.74, dropout rate 0.50\n",
      "Pruned to 169 neurons\n",
      "Batch 101, training loss 0.0104, val loss 0.0087, block_reg 0.74, dropout rate 0.50\n",
      "Batch 201, training loss 0.0115, val loss 0.0092, block_reg 0.74, dropout rate 0.50\n",
      "Batch 301, training loss 0.0100, val loss 0.0082, block_reg 0.74, dropout rate 0.50\n",
      "Pruned to 168 neurons\n",
      "Batch 401, training loss 0.0110, val loss 0.0090, block_reg 0.74, dropout rate 0.50\n",
      "Batch 501, training loss 0.0142, val loss 0.0109, block_reg 0.74, dropout rate 0.50\n",
      "Batch 601, training loss 0.0104, val loss 0.0081, block_reg 0.74, dropout rate 0.50\n",
      "Batch 701, training loss 0.0108, val loss 0.0079, block_reg 0.74, dropout rate 0.50\n",
      "Pruned to 167 neurons\n",
      "Batch 801, training loss 0.0094, val loss 0.0080, block_reg 0.74, dropout rate 0.50\n",
      "Batch 901, training loss 0.0126, val loss 0.0074, block_reg 0.74, dropout rate 0.50\n",
      "Batch 1001, training loss 0.0085, val loss 0.0071, block_reg 0.73, dropout rate 0.50\n",
      "Batch 1101, training loss 0.0080, val loss 0.0077, block_reg 0.73, dropout rate 0.50\n",
      "Pruned to 166 neurons\n",
      "Batch 1201, training loss 0.0110, val loss 0.0075, block_reg 0.73, dropout rate 0.50\n",
      "Batch 1301, training loss 0.0099, val loss 0.0082, block_reg 0.73, dropout rate 0.50\n",
      "Batch 1401, training loss 0.0100, val loss 0.0087, block_reg 0.73, dropout rate 0.50\n",
      "Batch 1501, training loss 0.0093, val loss 0.0082, block_reg 0.73, dropout rate 0.50\n",
      "Pruned to 165 neurons\n",
      "Batch 1601, training loss 0.0086, val loss 0.0074, block_reg 0.73, dropout rate 0.50\n",
      "Batch 1701, training loss 0.0090, val loss 0.0074, block_reg 0.73, dropout rate 0.50\n",
      "Batch 1801, training loss 0.0078, val loss 0.0075, block_reg 0.73, dropout rate 0.50\n",
      "Train epoch 7, loss 0.01014075220376253\n",
      "Batch 1, training loss 0.0082, val loss 0.0077, block_reg 0.73, dropout rate 0.50\n",
      "Pruned to 164 neurons\n",
      "Batch 101, training loss 0.0075, val loss 0.0074, block_reg 0.72, dropout rate 0.50\n",
      "Batch 201, training loss 0.0095, val loss 0.0080, block_reg 0.72, dropout rate 0.50\n",
      "Batch 301, training loss 0.0100, val loss 0.0067, block_reg 0.72, dropout rate 0.50\n",
      "Pruned to 163 neurons\n",
      "Batch 401, training loss 0.0113, val loss 0.0069, block_reg 0.72, dropout rate 0.50\n",
      "Batch 501, training loss 0.0100, val loss 0.0071, block_reg 0.72, dropout rate 0.50\n",
      "Batch 601, training loss 0.0118, val loss 0.0097, block_reg 0.72, dropout rate 0.50\n",
      "Batch 701, training loss 0.0099, val loss 0.0075, block_reg 0.72, dropout rate 0.50\n",
      "Pruned to 162 neurons\n",
      "Batch 801, training loss 0.0081, val loss 0.0083, block_reg 0.72, dropout rate 0.50\n",
      "Batch 901, training loss 0.0093, val loss 0.0076, block_reg 0.72, dropout rate 0.50\n",
      "Batch 1001, training loss 0.0099, val loss 0.0086, block_reg 0.72, dropout rate 0.50\n",
      "Batch 1101, training loss 0.0090, val loss 0.0077, block_reg 0.72, dropout rate 0.50\n",
      "Pruned to 161 neurons\n",
      "Batch 1201, training loss 0.0125, val loss 0.0107, block_reg 0.72, dropout rate 0.50\n",
      "Batch 1301, training loss 0.0132, val loss 0.0102, block_reg 0.73, dropout rate 0.50\n",
      "Batch 1401, training loss 0.0121, val loss 0.0079, block_reg 0.72, dropout rate 0.50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "%run autoenc_br.py --epochs {n_epochs} --connected_components {ncc} --dataset colmnist --regularizer svd --bn_size {bn_size} --n_workers 0\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# %run autoenc_baseline.py --epochs {n_epochs} --dataset colmnist --bn_size {bn_size} --n_workers 0\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_utils import show_dataset_examples\n",
    "show_dataset_examples(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_utils import get_test_sample\n",
    "test_examples = get_test_sample(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(trainer.train_loader)/trainer.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_utils import show_reconstruction\n",
    "show_reconstruction(test_examples, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncc = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import block_regularizer\n",
    "block_regularizer(trainer.model.encoder_output_layer, ncc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import layer_svd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "enc_layer = trainer.model.encoder_output_layer\n",
    "u, s, v = layer_svd(enc_layer)\n",
    "u = u.detach().cpu().numpy()\n",
    "s = s.detach().cpu().numpy()\n",
    "v = v.detach().cpu().numpy()\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(u[:, :ncc].transpose(1, 0), cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import compute_layer_blocks_out, compute_layer_blocks_in\n",
    "blocks = compute_layer_blocks_out(enc_layer, ncc)\n",
    "len(blocks)\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotting_utils import plot_reconstruction\n",
    "\n",
    "for c in range(max(blocks)+1):\n",
    "    plt.figure(figsize=(20, 20)) \n",
    "    plt.imshow([blocks.numpy()==c], cmap='magma')\n",
    "    enc_layer.turn_output_neurons_off(blocks==c)\n",
    "    \n",
    "    show_reconstruction(test_examples, trainer)\n",
    "    \n",
    "    print(enc_layer.out_mask)\n",
    "    enc_layer.turn_all_output_neurons_on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_blocked_weights(layer):\n",
    "    plt.figure(figsize=(20, 7))\n",
    "    blocks_in = compute_layer_blocks_in(layer, ncc)\n",
    "    blocks_out = compute_layer_blocks_out(layer, ncc)\n",
    "    plt.imshow(layer.weight[np.argsort(blocks_out)][:, np.argsort(blocks_in)].cpu().detach().numpy(), cmap='magma')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weight = np.zeros(enc_layer.weight.shape)\n",
    "blocks_out = compute_layer_blocks_out(enc_layer, ncc)\n",
    "blocks_in = compute_layer_blocks_in(enc_layer, ncc)\n",
    "for i in range(ncc):\n",
    "    mask = np.ones(enc_layer.weight.shape, dtype=bool)\n",
    "    mask[blocks_out!=i] = 0\n",
    "    mask[:, blocks_in!=i] = 0\n",
    "    new_weight[mask] = enc_layer.weight.detach().cpu()[mask]\n",
    "    \n",
    "plt.imshow(new_weight, cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_layer.weight = nn.Parameter(torch.tensor(new_weight).cuda().float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_blocked_weights(enc_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(blocks==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in range(ncc):\n",
    "    print(\"block {}\".format(c))\n",
    "    for n in np.where(blocks==c)[0]:\n",
    "        mask = torch.zeros(60, dtype=torch.float)\n",
    "        mask[n] = 1\n",
    "        print(n)\n",
    "        enc_layer.turn_output_neurons_off(mask)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            reconstruction = trainer.model(test_examples)\n",
    "        plot_reconstruction(test_examples.detach(), reconstruction.detach().cpu())\n",
    "        print(trainer.criterion(test_examples, reconstruction))\n",
    "        enc_layer.turn_all_output_neurons_on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relative_error = [None]*bn_size\n",
    "for c in range(ncc):\n",
    "    mask = torch.zeros(60, dtype=torch.float)\n",
    "    mask[blocks==c] = 1\n",
    "    enc_layer.turn_output_neurons_off(mask)\n",
    "    with torch.no_grad():\n",
    "        reconstruction = trainer.model(test_examples)\n",
    "    block_error = trainer.criterion(test_examples, reconstruction).item()\n",
    "    enc_layer.turn_all_output_neurons_on()\n",
    "        \n",
    "    #print(\"block {}\".format(c))\n",
    "    for n in np.where(blocks!=c)[0]:\n",
    "        mask = torch.zeros(60, dtype=torch.float)\n",
    "        mask[blocks==c] = 1\n",
    "        mask[n] = 1\n",
    "        #print(n)\n",
    "        #plt.figure(figsize=(20, 20)) \n",
    "        #plt.imshow(mask.unsqueeze(0), cmap='magma')\n",
    "        enc_layer.turn_output_neurons_off(mask)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            reconstruction = trainer.model(test_examples)\n",
    "        #plot_reconstruction(test_examples.detach(), reconstruction.detach().cpu())\n",
    "        relative_error[n] = block_error - trainer.criterion(test_examples, reconstruction).item()\n",
    "        enc_layer.turn_all_output_neurons_on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.hist(np.array(relative_error)[blocks==0])\n",
    "plt.figure()\n",
    "plt.hist(np.array(relative_error)[blocks==1])\n",
    "thr = np.percentile(np.array(relative_error), 20)\n",
    "re_mask = np.array([e>thr for e in relative_error])\n",
    "re_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(ncc):\n",
    "    mask = np.logical_and(np.array(blocks==c), re_mask)\n",
    "    plt.figure(figsize=(20, 20)) \n",
    "    plt.imshow(torch.tensor(mask).unsqueeze(0), cmap='magma')\n",
    "    enc_layer.turn_output_neurons_off(mask)\n",
    "    \n",
    "    show_reconstruction(test_examples, trainer)\n",
    "    enc_layer.turn_all_output_neurons_on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_enc = np.argsort(v.transpose(1, 0)[:, 1]).tolist()\n",
    "srt_hidden = np.argsort(u.transpose(1, 0)[:, 1]).tolist()\n",
    "\n",
    "trainer.model.set_mask(None)\n",
    "\n",
    "w1 = trainer.model.encoder_output_layer.weight.cpu().detach()\n",
    "w1 = w1[:, srt_enc]\n",
    "w1 = w1[srt_hidden, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20)) \n",
    "plt.imshow((w1).cpu().numpy(), cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_examples = [[[] for _ in range(3)] for _ in range(10)]\n",
    "for batch_features in trainer.test_loader:\n",
    "    batch_features, (digit, _, clr) = batch_features\n",
    "    for i in range(len(batch_features)):\n",
    "        binned_examples[digit[i].item()][clr[i].item()].append(batch_features[i].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(10):\n",
    "    for clr in range(3):\n",
    "        binned_examples[d][clr] = torch.stack(binned_examples[d][clr]).to(device)\n",
    "        print(binned_examples[d][clr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for _ in range(ncc):\n",
    "    losses.append(torch.zeros((3, 10)))\n",
    "    \n",
    "for c in range(ncc):\n",
    "    mask = torch.zeros(w.shape)\n",
    "    mask.shape\n",
    "    mask[blocks==c] = 1\n",
    "    plt.figure(figsize=(20, 20)) \n",
    "    plt.imshow(mask[:, :1].transpose(1, 0), cmap='magma')\n",
    "    trainer.model.set_mask(mask)\n",
    "    \n",
    "    for clr in range(3):\n",
    "        for d in range(10):\n",
    "            with torch.no_grad():\n",
    "                reconstruction = trainer.model(binned_examples[d][clr]).cpu()\n",
    "            show_reconstruction(binned_examples[d][clr], reconstruction)\n",
    "            losses[c][clr][d] = trainer.criterion(binned_examples[d][clr], reconstruction.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for c in range(ncc):\n",
    "    plt.grid(False)\n",
    "    plt.imshow(losses[c], cmap=\"magma\")\n",
    "    plt.show()\n",
    "    print(losses[c])\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses[0].sum(0))\n",
    "print(losses[0].sum(1))\n",
    "print(losses[1].sum(0))\n",
    "print(losses[1].sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_dropout(blocks, prob):\n",
    "    mask = torch.zeros(len(blocks), dtype=torch.bool)\n",
    "    first_mask = torch.rand(((blocks==0).sum(), )) < prob\n",
    "    mask[blocks==0] = first_mask\n",
    "    p = first_mask.sum()/len(first_mask)\n",
    "    for i in range(1, max(blocks)+1):\n",
    "        new_mask = torch.zeros(((blocks==i).sum(), ), dtype=torch.bool)\n",
    "        new_mask[:int(p*len(new_mask))] = 1\n",
    "        mask[blocks==i] = new_mask[torch.randperm(len(new_mask))]\n",
    "\n",
    "    return mask.float()\n",
    "\n",
    "block_dropout(blocks, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
    "for i in range(20):\n",
    "    d = block_dropout(b_test, 0.3)\n",
    "    print(d[:8].sum().item(), d[8:].sum().item())\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to visualize the outputs of PCA and t-SNE\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "RS = 123\n",
    "\n",
    "def fashion_scatter(x, colors):\n",
    "    # choose a color palette with seaborn.\n",
    "    num_classes = len(np.unique(colors))\n",
    "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # add the labels for each digit corresponding to the label\n",
    "    txts = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "\n",
    "        # Position of each label at median of data points.\n",
    "\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "targets = []\n",
    "d_targets = []\n",
    "c_targets = []\n",
    "for batch_features, (digit, _, clr) in trainer.test_loader:\n",
    "    with torch.no_grad():\n",
    "        reconstruction = trainer.model(batch_features).cpu()\n",
    "        embeddings.append(trainer.model.embedding)\n",
    "        d_targets.extend(digit.tolist())\n",
    "        c_targets.extend(clr.tolist())\n",
    "        #targets.extend([str(d)+\"_\"+str(c) for d, c in zip(digit.tolist(), clr.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb = torch.vstack(embeddings).detach().cpu().numpy()\n",
    "targets = [c*10+d for d, c in zip(d_targets, c_targets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(df_emb)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_scatter(tsne_results, np.array(targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_scatter(tsne_results, np.array(c_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_scatter(tsne_results, np.array(d_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results_block0 = tsne.fit_transform(df_emb[:, blocks==0])\n",
    "tsne_results_block1 = tsne.fit_transform(df_emb[:, blocks==1])\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_scatter(tsne_results_block0, np.array(c_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_scatter(tsne_results_block0, np.array(d_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_scatter(tsne_results_block1, np.array(c_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_scatter(tsne_results_block1, np.array(d_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies between blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.figure(figsize=(20, 20)) \n",
    "plt.imshow(np.cov(df_emb[:, blocks==0].transpose(1, 0)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20)) \n",
    "plt.imshow(np.cov(df_emb[:, blocks==1].transpose(1, 0)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corr = np.corrcoef(df_emb[:, blocks==0].transpose(1, 0), df_emb[:, blocks==1].transpose(1, 0))\n",
    "plt.figure(figsize=(20, 20)) \n",
    "plt.imshow(full_corr)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corr = np.corrcoef(df_emb[:, blocks==0].transpose(1, 0), df_emb[:, blocks==1].transpose(1, 0))\n",
    "plt.figure(figsize=(20, 20)) \n",
    "plt.imshow(full_corr)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dist = np.zeros((bn_size, bn_size))\n",
    "for i in range(bn_size):\n",
    "    for j in range(i, bn_size):\n",
    "        dist[i, j] = np.abs(full_corr[i]-full_corr[j]).sum()\n",
    "        dist[j, i] = np.abs(full_corr[i]-full_corr[j]).sum()\n",
    "        \n",
    "plt.figure(figsize=(20, 20)) \n",
    "plt.imshow(dist<5)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(20, 20)) \n",
    "plt.hist(dist.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainn_code.grab_functions import db_from_dat_with_labels, write_pic_as_sets\n",
    "\n",
    "activations = []\n",
    "test_data = []\n",
    "for batch_features, (digit, _, clr) in trainer.test_loader:\n",
    "    with torch.no_grad():\n",
    "        reconstruction = trainer.model(batch_features).cpu()\n",
    "        activations.append(trainer.model.layer_0_out.detach().cpu().numpy())\n",
    "        test_data.append(batch_features.detach().cpu().numpy())\n",
    "activations = np.concatenate(activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    write_pic_as_sets(activations[:, i, :].reshape(activations.shape[0], -1), \"filters_{}.dat\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.concatenate(test_data)\n",
    "write_pic_as_sets(test_data.reshape(test_data.shape[0], -1), \"test_data.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    db_from_dat_with_labels(\"filters_{}.dat\".format(i), \"test_data.dat\", \"data{}.db\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
